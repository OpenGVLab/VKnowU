#!/usr/bin/env python3
"""
vLLMæœåŠ¡å¯åŠ¨è„šæœ¬
ç”¨äºå¯åŠ¨QWEN3æ¨¡å‹çš„vLLMæ¨ç†æœåŠ¡
"""

import os
import sys
import argparse
from vllm import LLM, SamplingParams

def start_vllm_service():
    """å¯åŠ¨vLLMæœåŠ¡"""
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'
    
    print("æ­£åœ¨å¯åŠ¨vLLMæœåŠ¡...")
    print("æ¨¡å‹è·¯å¾„: /fs-computility/video/shared/hf_weight/Qwen3-32B")
    
    try:
        # åˆå§‹åŒ–vLLMæ¨¡å‹
        llm = LLM(
            model="/fs-computility/video/shared/hf_weight/Qwen3-32B",
            trust_remote_code=True,
            dtype="auto",
            gpu_memory_utilization=0.9,
            max_model_len=8192,
            enforce_eager=True,
        )
        
        print("âœ… vLLMæ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"æ¨¡å‹è®¾å¤‡: {llm.llm_engine.model_executor.driver_worker.model_runner.device}")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦: {llm.llm_engine.model_executor.driver_worker.model_runner.max_model_len}")
        
        # æµ‹è¯•æ¨ç†
        print("\næ­£åœ¨æµ‹è¯•æ¨ç†...")
        test_prompt = "Hello, how are you?"
        sampling_params = SamplingParams(temperature=0.0, max_tokens=10)
        outputs = llm.generate([test_prompt], sampling_params)
        response = outputs[0].outputs[0].text.strip()
        print(f"æµ‹è¯•è¾“å…¥: {test_prompt}")
        print(f"æµ‹è¯•è¾“å‡º: {response}")
        print("âœ… æ¨ç†æµ‹è¯•æˆåŠŸ!")
        
        return llm
        
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¯åŠ¨vLLMæœåŠ¡")
    parser.add_argument("--test", action="store_true", help="ä»…æµ‹è¯•æ¨¡å‹åŠ è½½")
    args = parser.parse_args()
    
    if args.test:
        llm = start_vllm_service()
        if llm:
            print("\nğŸ‰ vLLMæœåŠ¡æµ‹è¯•å®Œæˆï¼Œå¯ä»¥å¯åŠ¨Flaskåº”ç”¨äº†!")
        else:
            print("\nâŒ vLLMæœåŠ¡æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®!")
            sys.exit(1)
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("python start_vllm_service.py --test  # æµ‹è¯•vLLMæœåŠ¡")
        print("python qwen3_caption_service.py      # å¯åŠ¨Flaskåº”ç”¨") 